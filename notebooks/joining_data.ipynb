{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a visualization that you used as part of your EDA process. Explain the initial pattern or relationship you discoved through this visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining of dataframes has been done post competing EDA for each of the dataframes. Hence, the sections have been shuffled\n",
    "\n",
    "\n",
    "city_bike_df = pd.read_csv(r'C:\\Users\\aksha\\Documents\\LHL\\LHL Assignments\\Week 5\\Python Statistical Modeling Project\\City_Bike_Vancouver_Data_17th.csv')\n",
    "yelp_df = pd.read_csv(r'C:\\Users\\aksha\\Documents\\LHL\\LHL Assignments\\Week 5\\Python Statistical Modeling Project\\Yelp_Data_17th.csv')\n",
    "fs_df = pd.read_csv(r'C:\\Users\\aksha\\Documents\\LHL\\LHL Assignments\\Week 5\\Python Statistical Modeling Project\\Foursquare_Data_17th.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Audit and cleaning for City Bike dataframe\n",
    "city_bike_df.head()\n",
    "city_bike_df.shape # 248,13\n",
    "city_bike_df.info() # No need to change data types\n",
    "city_bike_df.describe(include = 'all')\n",
    "\n",
    "\n",
    "# Column wise EDA and data cleaning\n",
    "# ID can be dropped, since it's repetitive and was created for joining purposes\n",
    "# City and Country will be same for all the records hence it can be left as it is\n",
    "# Station Id - just a unique identifier for the station, can be left as it is\n",
    "# Station name - \n",
    "    # Scope for text cleaning\n",
    "    # Check distribution of values and see if there are some repetitive names\n",
    "    # Create geographical map of stations using the latitude and longitude for showing a density plot \n",
    "# Time stamp - It will be same for all, extract and retain the date in a cleaner format as per Vancouver timezone\n",
    "# For the remaining columns check the distribution and if anomalies are present (can be correlated with the time and day of api call for example all bikes shall be available at odd hours and so)\n",
    "\n",
    "# Other plots to be created post joining and additional columns to be created\n",
    "# Correlation heatmap to establish hypothesis\n",
    "# Popularity index of a station - with reference to review count, ratings and number of POIs\n",
    "# Correlation between popularity index and cycle renting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_bike_df = city_bike_df.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Names for DQ issues\n",
    "city_bike_df.Station_Name.value_counts() \n",
    "# all have a count of one which means no 2 stations ids have the same name \n",
    "\n",
    "sum(city_bike_df['Station_Name'].isnull() | city_bike_df['Station_Name'] == '')\n",
    "# No junk values found in the distribution like Null, NAN, Blanks, 0 etc\n",
    "\n",
    "# Text cleaning \n",
    "import re\n",
    "\n",
    "def process_location_string(location_string):\n",
    "    # Use regex to find the brackets and their content\n",
    "    match = re.search(r'^(.*?)\\(([^)]*)\\)(.*)$', location_string)\n",
    "\n",
    "    if match:\n",
    "        # Extract parts of the string before, between, and after brackets\n",
    "        before_brackets = match.group(1)\n",
    "        brackets_content = match.group(2)\n",
    "        after_brackets = match.group(3)\n",
    "\n",
    "        # Add \"near\" before the brackets and remove brackets\n",
    "        new_location_string = f\"{before_brackets}near {brackets_content}{after_brackets}\"\n",
    "\n",
    "        return new_location_string\n",
    "\n",
    "    # Return the original string if no match is found\n",
    "    return location_string\n",
    "\n",
    "city_bike_df['Station_Name'] = city_bike_df['Station_Name'].apply(process_location_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the bike stations in a geo map\n",
    "\n",
    "color_scale = [(0, 'orange'), (1,'red')]\n",
    "\n",
    "fig = px.scatter_mapbox(city_bike_df, \n",
    "                        lat=\"Latitude\", \n",
    "                        lon=\"Longitude\", \n",
    "                        hover_name=\"Station_Name\", \n",
    "                        hover_data=[\"Station_Name\", \"Total_Available_Free_Bikes\"],\n",
    "                        color=\"Total_Available_Free_Bikes\",\n",
    "                        color_continuous_scale=color_scale,\n",
    "                        size=\"Total_Available_Free_Bikes\",\n",
    "                        zoom=8, \n",
    "                        height=600,\n",
    "                        width=800)\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()\n",
    "# Majority of the stations are in the downtown area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Date and Time\n",
    "\n",
    "city_bike_df['Timestamp'].value_counts() # Millisconds were not in consistent format for 1 record in the timestamp object\n",
    "\n",
    "import pytz\n",
    "\n",
    "# Function to convert UTC to Vancouver time\n",
    "def convert_to_vancouver(timestamp):\n",
    "    utc_datetime = pd.to_datetime(timestamp, errors='coerce')\n",
    "    if not pd.isnull(utc_datetime):\n",
    "        vancouver_timezone = pytz.timezone(\"America/Vancouver\")\n",
    "        vancouver_datetime = utc_datetime.replace(tzinfo=pytz.utc).astimezone(vancouver_timezone)\n",
    "        vancouver_timestamp = vancouver_datetime.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3] + \"Z\"\n",
    "        return vancouver_timestamp\n",
    "    return None\n",
    "\n",
    "# Apply the conversion function to the 'Timestamp' column\n",
    "city_bike_df['Date_Time'] = city_bike_df['Timestamp'].apply(convert_to_vancouver)\n",
    "\n",
    "city_bike_df['Vancouver_Timestamp'] = pd.to_datetime(city_bike_df['Vancouver_Timestamp'])\n",
    "\n",
    "# Extract date and time components\n",
    "city_bike_df['Date'] = city_bike_df['Vancouver_Timestamp'].dt.date\n",
    "city_bike_df['Time'] = city_bike_df['Vancouver_Timestamp'].dt.time\n",
    "\n",
    "city_bike_df.drop(['Timestamp', 'Vancouver_Timestamp'], axis = 1, inplace = True)\n",
    "city_bike_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly we will check for anomalies in the bike and slots data using boxplots and study their distributions\n",
    "\n",
    "# Create a figure and a grid of subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 8))\n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Example boxplots for different columns\n",
    "sns.boxplot(y='Total_Available_Slots', data=city_bike_df, ax=axes[0])\n",
    "axes[0].set_title('Total_Available_Slots')\n",
    "\n",
    "sns.boxplot(y='Total_Available_Free_Bikes', data=city_bike_df, ax=axes[1])\n",
    "axes[1].set_title('Total_Available_Free_Bikes')\n",
    "\n",
    "sns.boxplot(y='Total_Available_EBikes', data=city_bike_df, ax=axes[2])\n",
    "axes[2].set_title('Total_Available_EBikes')\n",
    "\n",
    "sns.boxplot(y='Total_Available_Normal_Bikes', data=city_bike_df, ax=axes[3])\n",
    "axes[3].set_title('Total_Available_Normal_Bikes')\n",
    "\n",
    "sns.boxplot(y='Total_Available_Empty_Slots', data=city_bike_df, ax=axes[4])\n",
    "axes[4].set_title('Total_Available_Empty_Slots')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "# Following columns have anomalies - Total_Available_Slots, Total_Available_Free_Bikes, Total_Available_EBikes, Total_Available_Normal_Bikes\n",
    "# Skewness - All the boxplots are right skewed\n",
    "# If there had been any missing values then we could not have used mean to impute the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure and a grid of subplots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 8))\n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Example histograms with a line for different columns\n",
    "columns = ['Total_Available_Slots','Total_Available_Free_Bikes','Total_Available_EBikes',\n",
    "'Total_Available_Normal_Bikes','Total_Available_Empty_Slots']\n",
    "\n",
    "for i, column in enumerate(columns):\n",
    "    sns.histplot(city_bike_df[column].dropna(), kde=True, ax=axes[i], color='skyblue', bins=20)\n",
    "    axes[i].set_title(f'Histogram for {column}')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 stations with the most number of bikes in use\n",
    "city_bike_df.head()\n",
    "city_bike_df['Total_Bikes_In_Use'] = city_bike_df['Total_Available_Slots'] - city_bike_df['Total_Available_Free_Bikes']\n",
    "city_bike_df.iloc[city_bike_df['Total_Bikes_In_Use'].value_counts().head(5).index]['Station_Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data from Part 1 with the data from Part 2 to create a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the dataframes\n",
    "\n",
    "print(city_bike_df.shape)\n",
    "print(yelp_df.shape)\n",
    "print(fs_df.shape)\n",
    "\n",
    "# Check column names and data types\n",
    "\n",
    "print(city_bike_df.info())\n",
    "print(yelp_df.info())\n",
    "print(fs_df.info()) # Lat and Long are in float\n",
    "\n",
    "# Check for overalpping records\n",
    "\n",
    "print(sum(yelp_df.Latitude.isin(city_bike_df.Latitude)))\n",
    "print(sum(yelp_df.Longitude.isin(city_bike_df.Longitude)))\n",
    "\n",
    "print(sum(fs_df.Latitude.isin(city_bike_df.Latitude)))\n",
    "print(sum(fs_df.Longitude.isin(city_bike_df.Longitude)))\n",
    "\n",
    "complete_df = pd.merge(city_bike_df, yelp_df, on = ['Latitude', 'Longitude'], how = 'left')\n",
    "print(complete_df.shape)\n",
    "complete_df = pd.merge(complete_df, fs_df, on = ['Latitude', 'Longitude'], how = 'left')\n",
    "print(complete_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA Part Continued post joining the dataframes\n",
    "\n",
    "# Data cleaning and EDA steps\n",
    "# Drop id_dummy column\n",
    "# Rename Yelp columns and FS columns for easier bifurcation\n",
    "# Check distribution and anomalies in newly added columns\n",
    "# Impute NA values\n",
    "# Create new columns - Total Number of FSQ POIs, Avg number of POI in a place, Popularity index of a place (3 buckets as per the count of POIs)\n",
    "# Geo map of POI per staion\n",
    "# Check for correlation between following parameters -\n",
    "    # Total Slots and Total POIs\n",
    "    # Total Bikes in Use and Total POIs\n",
    "    # Total Bikes in Use and Total number of reviews\n",
    "    # Total Bikes in Use and Avg Ratings\n",
    "\n",
    "complete_df.drop('id_dummy', axis = 1, inplace = True)\n",
    "\n",
    "column_mapping = {'Total_POI': 'Yelp_Total_POI', 'Closed_POI': 'Yelp_Closed_POI', 'AvgRating': 'Yelp_Avg_Rating',\n",
    "                 'AvgReviewCount': 'Yelp_Avg_Review_Count', 'PriceCategory': 'Yelp_Price_Category', 'AvgRating': 'Yelp_Avg_Rating',\n",
    "                 'LikelyOpen': 'FSQ_Likely_Open', 'VeryLikelyOpen': 'FSQ_Very_Likely_Open', 'Unsure': 'FSQ_Unsure'}\n",
    "\n",
    "# Rename columns using the 'rename' method\n",
    "complete_df.rename(columns=column_mapping, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and EDA steps\n",
    "# Drop id_dummy column\n",
    "# Rename Yelp columns and FS columns for easier bifurcation\n",
    "# Check distribution and anomalies in newly added columns\n",
    "# Impute NA values\n",
    "# Create new columns - Total Number of FSQ POIs, Avg number of POI in a place, Popularity index of a place (3 buckets as per the count of POIs)\n",
    "# Geo map of POI per staion\n",
    "# Check for correlation between following parameters -\n",
    "    # Total Slots and Total POIs\n",
    "    # Total Bikes in Use and Total POIs\n",
    "    # Total Bikes in Use and Total number of reviews\n",
    "    # Total Bikes in Use and Avg Ratings\n",
    "\n",
    "complete_df.drop('id_dummy', axis = 1, inplace = True)\n",
    "\n",
    "column_mapping = {'Total_POI': 'Yelp_Total_POI', 'Closed_POI': 'Yelp_Closed_POI', 'AvgRating': 'Yelp_Avg_Rating',\n",
    "                 'AvgReviewCount': 'Yelp_Avg_Review_Count', 'PriceCategory': 'Yelp_Price_Category', 'AvgRating': 'Yelp_Avg_Rating',\n",
    "                 'LikelyOpen': 'FSQ_Likely_Open', 'VeryLikelyOpen': 'FSQ_Very_Likely_Open', 'Unsure': 'FSQ_Unsure'}\n",
    "\n",
    "# Rename columns using the 'rename' method\n",
    "complete_df.rename(columns=column_mapping, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Yelp Data Points\n",
    "\n",
    "# Create a figure and a grid of subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Example histograms with a line for different columns\n",
    "columns = ['Yelp_Total_POI','Yelp_Closed_POI','Yelp_Avg_Rating',\n",
    "'Yelp_Avg_Review_Count']\n",
    "\n",
    "for i, column in enumerate(columns):\n",
    "    sns.histplot(complete_df[column].dropna(), kde=True, ax=axes[i], color='skyblue', bins=20)\n",
    "    axes[i].set_title(f'Histogram for {column}')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "# Total POI is left skewed\n",
    "# Closed POI has only 0s\n",
    "# Avg Rating is slightly left skewed\n",
    "# Avg Review Count is right skewed\n",
    "\n",
    "# Since the Closed POI has only 0s we will be dropping it. \n",
    "\n",
    "complete_df.drop('Yelp_Closed_POI', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the frequency count in price category column\n",
    "\n",
    "# Get the count of each category\n",
    "price_category_counts = complete_df['Yelp_Price_Category'].value_counts()\n",
    "\n",
    "# Create a pie chart with percentage annotations\n",
    "plt.pie(price_category_counts, labels=price_category_counts.index, \n",
    "        autopct='%1.1f%%', startangle=90, colors=sns.color_palette('pastel'))\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA Imputation in Yelp Data\n",
    "# There is one record in the dataframe where the records were not found in api response. \n",
    "# After viewing the distribution, owing to skewness in data we will be imputing with median values\n",
    "# In categorical column we will be imputing with the mode of the column\n",
    "\n",
    "\n",
    "# Calculate median for 'Yelp_Total_POI','Yelp_Avg_Rating','Yelp_Avg_Review_Count'\n",
    "median_values = complete_df[['Yelp_Total_POI','Yelp_Avg_Rating','Yelp_Avg_Review_Count']].median()\n",
    "\n",
    "# Calculate mode for col4\n",
    "mode_value = complete_df['Yelp_Price_Category'].mode().iloc[0]\n",
    "\n",
    "# # Impute missing values using fillna\n",
    "complete_df['Yelp_Total_POI'].fillna(median_values['Yelp_Total_POI'], inplace=True)\n",
    "complete_df['Yelp_Avg_Rating'].fillna(median_values['Yelp_Avg_Rating'], inplace=True)\n",
    "complete_df['Yelp_Avg_Review_Count'].fillna(median_values['Yelp_Avg_Review_Count'], inplace=True)\n",
    "complete_df['Yelp_Price_Category'].fillna(mode_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation of NANs in the FSQ fields\n",
    "\n",
    "complete_df['FSQ_Likely_Open'].fillna(0, inplace=True)\n",
    "complete_df['FSQ_Very_Likely_Open'].fillna(0, inplace=True)\n",
    "complete_df['FSQ_Unsure'].fillna(0, inplace=True)\n",
    "\n",
    "# Check for NANs\n",
    "complete_df.isna().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create New column of Total FSQ POIs\n",
    "complete_df['FSQ_Total_POI'] = complete_df['FSQ_Likely_Open'] + complete_df['FSQ_Very_Likely_Open'] + complete_df['FSQ_Unsure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution of FSQ POI\n",
    "complete_df.head(25)\n",
    "complete_df['FSQ_Total_POI'].value_counts() # We can see that FSQ has not given more than 10 records per lat long whereas in Yelp\n",
    "# API response the range was much wider \n",
    "\n",
    "sns.histplot(complete_df['FSQ_Total_POI'], bins=20, kde=True, color='skyblue')\n",
    "plt.title('Histogram of FSQ_Total_POI')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# the distribution is highly right-skewed, majority of values are concentrated around 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for variance in POI data per Lat Long between FSQ and Yelp data\n",
    "complete_df['Diff_POI_Count'] = complete_df['Yelp_Total_POI'] - complete_df['FSQ_Total_POI']\n",
    "sns.histplot(complete_df['Diff_POI_Count'], bins=20, kde=True, color='skyblue')\n",
    "plt.title('Histogram of Diff_POI_Count')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "complete_df[['Yelp_Total_POI', 'FSQ_Total_POI']].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yelp data seems to be more complete and thus reliable compared to the FSQ data. \n",
    "# Therefore we will be considering a weighted avg of both the data sources - 90% weightage to Yelp and 10% to FSQ\n",
    "\n",
    "complete_df['Avg_POI_Count'] = 0.9 * complete_df['Yelp_Total_POI'] + 0.1 * complete_df['FSQ_Total_POI']\n",
    "# Since the sum of weights is 1 we won't be dividing with it.\n",
    "\n",
    "# Popularity Index\n",
    "\n",
    "# Specify the number of bins (in this case, 3 for low, medium, and high)\n",
    "num_bins = 3\n",
    "\n",
    "# Use qcut to bucket the numerical column into three categories\n",
    "complete_df['Popularity_Index'] = pd.qcut(complete_df['Avg_POI_Count'], q=num_bins, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "\n",
    "# Visualize the bike stations in a geo map\n",
    "\n",
    "color_scale = {'Low': 'orange', 'Medium': 'yellow', 'High': 'red'}\n",
    "\n",
    "# Create a numerical representation of 'Popularity_Index'\n",
    "complete_df['Popularity_Index_Num'] = complete_df['Popularity_Index'].map({'Low': 1, 'Medium': 2, 'High': 3})\n",
    "\n",
    "fig = px.scatter_mapbox(complete_df, \n",
    "                        lat=\"Latitude\", \n",
    "                        lon=\"Longitude\", \n",
    "                        hover_name=\"Station_Name\", \n",
    "                        hover_data=[\"Station_Name\", \"Avg_POI_Count\"],\n",
    "                        color=\"Popularity_Index_Num\",\n",
    "                        color_discrete_map=color_scale,\n",
    "                        size=\"Popularity_Index_Num\",\n",
    "                        zoom=8, \n",
    "                        height=800,\n",
    "                        width=800,\n",
    "                        labels={'Popularity_Index_Num': 'Popularity Index'})  # Title for the legend\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()\n",
    "# The further we go from downtown the lesser is the popularity index of the station\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for correlation between following parameters -\n",
    "    # Total Slots and Total POIs\n",
    "    # Total Bikes in Use and Total POIs\n",
    "    # Total Bikes in Use and Total number of reviews\n",
    "    # Total Bikes in Use and Avg Ratings\n",
    "    \n",
    "complete_df.columns\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = complete_df[['Total_Available_Slots', 'Yelp_Avg_Rating', 'Yelp_Avg_Review_Count', 'Avg_POI_Count', 'Total_Bikes_In_Use']].corr()\n",
    "\n",
    "# Create a heatmap \n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Review count and ratings are highly correlated (0.46)\n",
    "# Avg POI count and review count are highly correlated (0.58)\n",
    "# Since one is a subset of another, hence it's obvious that total Bikes in Use and Available slots are mildly correlated (0.33)\n",
    "\n",
    "# DQ Check \n",
    "# Check whether the Empty slots = Total Slots - Total Free Bikes\n",
    "sum(complete_df['Total_Available_Empty_Slots'] == complete_df['Total_Available_Slots'] - complete_df['Total_Available_Free_Bikes'])\n",
    "# In 194 records the total available empty slots matches the difference between available slots and available free bikes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_df.columns)\n",
    "# Box plot of total bikes in use vs price category\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Yelp_Price_Category', y='Total_Bikes_In_Use', data=complete_df)\n",
    "plt.title('Boxplot of Total_Bikes_In_Use by Yelp_Price_Category')\n",
    "plt.show()\n",
    "\n",
    "# We can see that the total bikes in use are in higher number within an economical POI compared to an Expensive one\n",
    "# We will validate the hypothesis using a t test\n",
    "\n",
    "# T Test (Welch's t test as both the groups have unequal sample sizes) \n",
    "\n",
    "# Separate data into two groups\n",
    "economical_category = complete_df['Total_Bikes_In_Use'][complete_df['Yelp_Price_Category'] == 'Economical']\n",
    "expensive_category = complete_df['Total_Bikes_In_Use'][complete_df['Yelp_Price_Category'] == 'Expensive']\n",
    "\n",
    "# Perform Welch's t-test\n",
    "t_statistic, p_value = ttest_ind(economical_category, expensive_category, equal_var=False)\n",
    "\n",
    "# Print the t-test result\n",
    "print(f\"T-Statistic: {t_statistic}\")\n",
    "print(f\"P-Value: {p_value}\")\n",
    "\n",
    "# Check the p-value to determine statistical significance\n",
    "if p_value < 0.05:\n",
    "    print(\"The means are significantly different.\")\n",
    "else:\n",
    "    print(\"No significant difference in means.\")\n",
    "\n",
    "# We cannot reject the null hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all your results in an SQLite3 database (remember, SQLite stores its databases as files in your local machine - make sure to create your database in your project's data/ directory!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQLite database \n",
    "db_path = 'bike_station.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a table\n",
    "create_table_query = '''\n",
    "    CREATE TABLE IF NOT EXISTS bike_station_data (\n",
    "        City TEXT NOT NULL,\n",
    "        Country TEXT NOT NULL,\n",
    "        Station_Id TEXT NOT NULL,\n",
    "        Station_Name TEXT NOT NULL,\n",
    "        Latitude REAL NOT NULL,\n",
    "        Longitude REAL NOT NULL,\n",
    "        Total_Available_Slots INTEGER NOT NULL,\n",
    "        Total_Available_Free_Bikes INTEGER NOT NULL,\n",
    "        Total_Available_EBikes INTEGER NOT NULL,\n",
    "        Total_Available_Normal_Bikes INTEGER NOT NULL,\n",
    "        Total_Available_Empty_Slots INTEGER NOT NULL,\n",
    "        Date_Time TIMESTAMP NOT NULL,\n",
    "        Total_Bikes_In_Use INTEGER NOT NULL,\n",
    "        Yelp_Total_POI REAL NOT NULL,\n",
    "        Yelp_Avg_Rating REAL NOT NULL,\n",
    "        Yelp_Avg_Review_Count REAL NOT NULL,\n",
    "        Yelp_Price_Category TEXT NOT NULL,\n",
    "        FSQ_Likely_Open REAL NOT NULL,\n",
    "        FSQ_Very_Likely_Open REAL NOT NULL,\n",
    "        FSQ_Unsure REAL NOT NULL,\n",
    "        FSQ_Total_POI REAL NOT NULL,\n",
    "        Diff_POI_Count REAL NOT NULL,\n",
    "        Avg_POI_Count REAL NOT NULL,\n",
    "        Popularity_Index TEXT NOT NULL,\n",
    "        Popularity_Index_Num TEXT NOT NULL\n",
    "    );\n",
    "'''\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Push the DataFrame into the table\n",
    "complete_df.to_sql('bike_station_data', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Create an SQLite database (this will create the database file if it doesn't exist)\n",
    "db_path = 'bike_station.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Perform a SELECT * query\n",
    "select_query = 'SELECT * FROM bike_station_data limit 5'\n",
    "result_df = pd.read_sql(select_query, conn)\n",
    "\n",
    "# Print the result DataFrame\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data before and after the join to validate your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(complete_df.shape[0] == city_bike_df.shape[0])\n",
    "# Yes it's matching"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
